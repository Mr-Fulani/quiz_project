#!/usr/bin/env python3
"""
–ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π SEO-–∞—É–¥–∏—Ç —Å–∞–π—Ç–∞ quiz-code.com

–°–∫—Ä–∏–ø—Ç –ø—Ä–æ–≤–µ—Ä—è–µ—Ç:
- –†–µ–¥–∏—Ä–µ–∫—Ç—ã (301, 302, 307, 308) –∏ —Ü–µ–ø–æ—á–∫–∏ —Ä–µ–¥–∏—Ä–µ–∫—Ç–æ–≤
- –î–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –≤—Å–µ—Ö URL –∏–∑ sitemap.xml
- –ü—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å canonical URLs
- Hreflang —Ç–µ–≥–∏
- robots.txt
- –ú–µ—Ç–∞-—Ç–µ–≥–∏ (robots, canonical)
- HTTP —Å—Ç–∞—Ç—É—Å—ã
- –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –¥–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç —Å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º–∏
"""
from __future__ import annotations

import argparse
import sys
import json
from dataclasses import dataclass, asdict
from typing import List, Dict, Optional, Tuple
from urllib.parse import urljoin, urlparse
from datetime import datetime
import re

import requests
from bs4 import BeautifulSoup
from requests import Response
import xml.etree.ElementTree as ET

BASE_URL = "https://quiz-code.com"
SITEMAP_PATH = "/sitemap.xml"
ROBOTS_PATH = "/robots.txt"
DEFAULT_TIMEOUT = 15
MAX_REDIRECT_CHAIN = 5  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Ü–µ–ø–æ—á–∫–∏ —Ä–µ–¥–∏—Ä–µ–∫—Ç–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
MAX_URLS_PER_CATEGORY = 10  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ URL –∏–∑ –∫–∞–∂–¥–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏

EXPECTED_LANGS = {"en", "ru", "x-default"}


@dataclass
class RedirectInfo:
    """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ä–µ–¥–∏—Ä–µ–∫—Ç–µ"""
    url: str
    status_code: int
    location: Optional[str] = None
    is_permanent: bool = False
    redirect_type: str = ""  # "301", "302", "307", "308"


@dataclass
class RedirectChain:
    """–¶–µ–ø–æ—á–∫–∞ —Ä–µ–¥–∏—Ä–µ–∫—Ç–æ–≤"""
    start_url: str
    final_url: str
    chain: List[RedirectInfo]
    is_too_long: bool = False
    has_302: bool = False


@dataclass
class PageCheckResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
    url: str
    status_code: int
    has_canonical: bool = False
    canonical_url: Optional[str] = None
    canonical_issues: List[str] = None
    hreflang_tags: Dict[str, str] = None  # lang -> url
    missing_hreflang: List[str] = None
    has_robots_meta: bool = False
    robots_content: Optional[str] = None
    redirect_chain: Optional[RedirectChain] = None
    errors: List[str] = None

    def __post_init__(self):
        if self.canonical_issues is None:
            self.canonical_issues = []
        if self.hreflang_tags is None:
            self.hreflang_tags = {}
        if self.missing_hreflang is None:
            self.missing_hreflang = []
        if self.errors is None:
            self.errors = []


@dataclass
class AuditResult:
    """–û–±—â–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∞—É–¥–∏—Ç–∞"""
    timestamp: str
    base_url: str
    robots_check: Dict
    sitemap_check: Dict
    redirect_issues: List[RedirectChain]
    pages_check: List[PageCheckResult]
    summary: Dict
    recommendations: List[str]


class SEOAuditor:
    """–ö–ª–∞—Å—Å –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è SEO-–∞—É–¥–∏—Ç–∞"""

    def __init__(self, base_url: str = BASE_URL, timeout: int = DEFAULT_TIMEOUT):
        self.base_url = base_url.rstrip('/')
        self.timeout = timeout
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (compatible; SEO-Audit-Bot/1.0)'
        })

    def fetch_with_redirects(self, path: str, follow: bool = True) -> Tuple[Response, Optional[RedirectChain]]:
        """
        –ü–æ–ª—É—á–∞–µ—Ç URL —Å –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ–º —Ä–µ–¥–∏—Ä–µ–∫—Ç–æ–≤
        
        Returns:
            Tuple[Response, Optional[RedirectChain]]: –û—Ç–≤–µ—Ç –∏ —Ü–µ–ø–æ—á–∫–∞ —Ä–µ–¥–∏—Ä–µ–∫—Ç–æ–≤
        """
        url = urljoin(self.base_url, path)
        redirect_chain = None
        
        if not follow:
            # –û—Ç—Å–ª–µ–∂–∏–≤–∞–µ–º —Ä–µ–¥–∏—Ä–µ–∫—Ç—ã –≤—Ä—É—á–Ω—É—é
            redirect_chain = RedirectChain(
                start_url=url,
                final_url=url,
                chain=[]
            )
            
            current_url = url
            visited_urls = set()
            
            for _ in range(MAX_REDIRECT_CHAIN):
                if current_url in visited_urls:
                    break
                visited_urls.add(current_url)
                
                try:
                    resp = self.session.get(
                        current_url,
                        timeout=self.timeout,
                        allow_redirects=False
                    )
                except Exception as e:
                    return resp if 'resp' in locals() else None, redirect_chain
                
                redirect_chain.final_url = current_url
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –æ—Ç–≤–µ—Ç —Ä–µ–¥–∏—Ä–µ–∫—Ç–æ–º
                if resp.status_code in [301, 302, 307, 308]:
                    redirect_info = RedirectInfo(
                        url=current_url,
                        status_code=resp.status_code,
                        location=resp.headers.get('Location'),
                        is_permanent=resp.status_code in [301, 308],
                        redirect_type=str(resp.status_code)
                    )
                    redirect_chain.chain.append(redirect_info)
                    
                    if redirect_info.status_code == 302:
                        redirect_chain.has_302 = True
                    
                    # –ü–µ—Ä–µ—Ö–æ–¥–∏–º –∫ —Å–ª–µ–¥—É—é—â–µ–º—É URL
                    location = redirect_info.location
                    if not location:
                        break
                    
                    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ URL
                    if location.startswith('/'):
                        parsed = urlparse(current_url)
                        location = f"{parsed.scheme}://{parsed.netloc}{location}"
                    elif not location.startswith('http'):
                        location = urljoin(current_url, location)
                    
                    current_url = location
                else:
                    # –ù–µ —Ä–µ–¥–∏—Ä–µ–∫—Ç - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—Ç–≤–µ—Ç
                    redirect_chain.final_url = current_url
                    return resp, redirect_chain
            
            # –¶–µ–ø–æ—á–∫–∞ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω–∞—è
            redirect_chain.is_too_long = len(redirect_chain.chain) >= MAX_REDIRECT_CHAIN
            # –ü–æ—Å–ª–µ–¥–Ω–∏–π –∑–∞–ø—Ä–æ—Å –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞
            try:
                resp = self.session.get(current_url, timeout=self.timeout, allow_redirects=True)
            except Exception as e:
                resp = Response()
                resp.status_code = 0
            return resp, redirect_chain
        else:
            try:
                resp = self.session.get(url, timeout=self.timeout, allow_redirects=True)
                return resp, None
            except Exception as e:
                resp = Response()
                resp.status_code = 0
                return resp, None

    def check_robots(self) -> Dict:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç robots.txt"""
        result = {
            'ok': True,
            'status_code': None,
            'errors': [],
            'content': None,
            'has_sitemap': False,
            'sitemap_url': None
        }
        
        try:
            resp, _ = self.fetch_with_redirects(ROBOTS_PATH, follow=True)
            result['status_code'] = resp.status_code
            
            if resp.status_code != 200:
                result['ok'] = False
                result['errors'].append(f"robots.txt –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç—É—Å {resp.status_code}")
                return result
            
            result['content'] = resp.text
            lines = [line.strip() for line in resp.text.splitlines() if line.strip()]
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ sitemap
            sitemap_lines = [line for line in lines if line.lower().startswith("sitemap:")]
            if sitemap_lines:
                result['has_sitemap'] = True
                result['sitemap_url'] = sitemap_lines[0].split(":", 1)[1].strip()
            else:
                result['ok'] = False
                result['errors'].append("–ù–µ –Ω–∞–π–¥–µ–Ω–∞ —Å—Ç—Ä–æ–∫–∞ Sitemap –≤ robots.txt")
        
        except Exception as e:
            result['ok'] = False
            result['errors'].append(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ robots.txt: {e}")
        
        return result

    def get_sitemap_urls(self) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –≤—Å–µ URL –∏–∑ sitemap.xml"""
        urls = []
        
        try:
            resp, _ = self.fetch_with_redirects(SITEMAP_PATH, follow=True)
            if resp.status_code != 200:
                return urls
            
            root = ET.fromstring(resp.text)
            ns = {
                "sm": "http://www.sitemaps.org/schemas/sitemap/0.9",
                "xhtml": "http://www.w3.org/1999/xhtml",
            }
            
            url_elements = root.findall("sm:url", ns)
            for url_el in url_elements:
                loc_el = url_el.find("sm:loc", ns)
                if loc_el is not None and loc_el.text:
                    urls.append(loc_el.text)
        
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ sitemap: {e}", file=sys.stderr)
        
        return urls

    def check_sitemap(self) -> Dict:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç sitemap.xml"""
        result = {
            'ok': True,
            'status_code': None,
            'errors': [],
            'url_count': 0,
            'has_http_links': False,
            'http_links': [],
            'missing_hreflang': set()
        }
        
        try:
            resp, _ = self.fetch_with_redirects(SITEMAP_PATH, follow=True)
            result['status_code'] = resp.status_code
            
            if resp.status_code != 200:
                result['ok'] = False
                result['errors'].append(f"sitemap.xml –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç—É—Å {resp.status_code}")
                return result
            
            root = ET.fromstring(resp.text)
            ns = {
                "sm": "http://www.sitemaps.org/schemas/sitemap/0.9",
                "xhtml": "http://www.w3.org/1999/xhtml",
            }
            
            url_elements = root.findall("sm:url", ns)
            result['url_count'] = len(url_elements)
            
            for url_el in url_elements:
                loc_el = url_el.find("sm:loc", ns)
                if loc_el is None:
                    continue
                
                loc_text = loc_el.text or ""
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ http:// —Å—Å—ã–ª–∫–∏
                if loc_text.startswith("http://"):
                    result['has_http_links'] = True
                    result['http_links'].append(loc_text)
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º hreflang
                alternates = url_el.findall("xhtml:link", ns)
                langs = {alt.get("hreflang") for alt in alternates if alt.get("hreflang")}
                missing = EXPECTED_LANGS - langs
                if missing:
                    result['missing_hreflang'].update(missing)
            
            result['missing_hreflang'] = list(result['missing_hreflang'])
            
            if result['has_http_links']:
                result['ok'] = False
                result['errors'].append(f"–ù–∞–π–¥–µ–Ω—ã http:// —Å—Å—ã–ª–∫–∏ –≤ sitemap ({len(result['http_links'])} —à—Ç.)")
            
            if result['missing_hreflang']:
                result['ok'] = False
                result['errors'].append(f"–ù–µ –¥–ª—è –≤—Å–µ—Ö URL –µ—Å—Ç—å hreflang —Ç–µ–≥–∏: {', '.join(sorted(result['missing_hreflang']))}")
        
        except Exception as e:
            result['ok'] = False
            result['errors'].append(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ sitemap.xml: {e}")
        
        return result

    def check_page(self, url: str, check_redirects: bool = True) -> PageCheckResult:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –æ—Ç–¥–µ–ª—å–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
        
        Args:
            url: –ü–æ–ª–Ω—ã–π URL –∏–ª–∏ –ø—É—Ç—å
            check_redirects: –ü—Ä–æ–≤–µ—Ä—è—Ç—å –ª–∏ —Ä–µ–¥–∏—Ä–µ–∫—Ç—ã
        """
        # –ï—Å–ª–∏ URL –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π, –¥–µ–ª–∞–µ–º –µ–≥–æ –ø–æ–ª–Ω—ã–º
        if not url.startswith('http'):
            url = urljoin(self.base_url, url)
        
        result = PageCheckResult(url=url, status_code=0)
        
        try:
            if check_redirects:
                resp, redirect_chain = self.fetch_with_redirects(url, follow=False)
            else:
                resp, redirect_chain = self.fetch_with_redirects(url, follow=True)
            
            result.status_code = resp.status_code
            
            if redirect_chain and redirect_chain.chain:
                result.redirect_chain = redirect_chain
            
            if resp.status_code != 200:
                result.errors.append(f"–°—Ç–∞—Ç—É—Å –∫–æ–¥: {resp.status_code}")
                return result
            
            # –ü–∞—Ä—Å–∏–º HTML
            soup = BeautifulSoup(resp.text, "html.parser")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º canonical
            canonical = soup.find("link", rel="canonical")
            if canonical and canonical.get("href"):
                result.has_canonical = True
                result.canonical_url = canonical["href"]
                
                # –ü—Ä–æ–≤–µ—Ä–∫–∏ canonical URL
                if result.canonical_url.startswith("http://"):
                    result.canonical_issues.append("Canonical –∏—Å–ø–æ–ª—å–∑—É–µ—Ç http:// –≤–º–µ—Å—Ç–æ https://")
                
                parsed_canonical = urlparse(result.canonical_url)
                parsed_current = urlparse(url)
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ canonical –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –æ—Å–Ω–æ–≤–Ω–æ–π –¥–æ–º–µ–Ω
                if 'mini.' in parsed_canonical.netloc:
                    result.canonical_issues.append(f"Canonical –∏—Å–ø–æ–ª—å–∑—É–µ—Ç mini –¥–æ–º–µ–Ω: {parsed_canonical.netloc}")
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ canonical —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π URL (–±–µ–∑ –ª–∏—à–Ω–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤)
                canonical_path = parsed_canonical.path
                current_path = parsed_current.path
                if canonical_path != current_path:
                    # –≠—Ç–æ –Ω–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –æ—à–∏–±–∫–∞, –Ω–æ —Å—Ç–æ–∏—Ç –ø—Ä–æ–≤–µ—Ä–∏—Ç—å
                    pass
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º hreflang
            hreflang_links = soup.find_all("link", rel="alternate", hreflang=True)
            for link in hreflang_links:
                lang = link.get("hreflang")
                href = link.get("href")
                if lang and href:
                    result.hreflang_tags[lang] = href
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ —è–∑—ã–∫–∏
            found_langs = set(result.hreflang_tags.keys())
            result.missing_hreflang = list(EXPECTED_LANGS - found_langs)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º robots meta —Ç–µ–≥
            robots_meta = soup.find("meta", attrs={"name": "robots"})
            if robots_meta:
                result.has_robots_meta = True
                result.robots_content = robots_meta.get("content", "")
        
        except Exception as e:
            result.errors.append(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {e}")
        
        return result

    def check_redirects_for_key_urls(self) -> List[RedirectChain]:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Ä–µ–¥–∏—Ä–µ–∫—Ç—ã –¥–ª—è –∫–ª—é—á–µ–≤—ã—Ö URL"""
        key_urls = [
            '/',  # –ö–æ—Ä–Ω–µ–≤–æ–π URL
            '/post/test-slug/',  # –ü–æ—Å—Ç –±–µ–∑ —è–∑—ã–∫–æ–≤–æ–≥–æ –ø—Ä–µ—Ñ–∏–∫—Å–∞ (–µ—Å–ª–∏ –µ—Å—Ç—å —Ç–∞–∫–æ–π)
        ]
        
        issues = []
        
        for path in key_urls:
            url = urljoin(self.base_url, path)
            resp, redirect_chain = self.fetch_with_redirects(path, follow=False)
            
            if redirect_chain and redirect_chain.chain:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –ø—Ä–æ–±–ª–µ–º—ã
                if redirect_chain.has_302:
                    issues.append(redirect_chain)
                if redirect_chain.is_too_long:
                    issues.append(redirect_chain)
                if len(redirect_chain.chain) > 2:  # –¶–µ–ø–æ—á–∫–∞ –±–æ–ª–µ–µ 2 —Ä–µ–¥–∏—Ä–µ–∫—Ç–æ–≤
                    issues.append(redirect_chain)
        
        return issues

    def run_audit(self, max_urls: int = MAX_URLS_PER_CATEGORY) -> AuditResult:
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç –ø–æ–ª–Ω—ã–π –∞—É–¥–∏—Ç"""
        print("üîç –ó–∞–ø—É—Å–∫ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ SEO-–∞—É–¥–∏—Ç–∞...")
        print(f"üìç –ë–∞–∑–æ–≤—ã–π URL: {self.base_url}\n")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ robots.txt
        print("üìã –ü—Ä–æ–≤–µ—Ä–∫–∞ robots.txt...")
        robots_check = self.check_robots()
        if robots_check['ok']:
            print("   ‚úÖ robots.txt –≤ –ø–æ—Ä—è–¥–∫–µ")
        else:
            print(f"   ‚ùå –ü—Ä–æ–±–ª–µ–º—ã: {', '.join(robots_check['errors'])}")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ sitemap.xml
        print("\nüìã –ü—Ä–æ–≤–µ—Ä–∫–∞ sitemap.xml...")
        sitemap_check = self.check_sitemap()
        print(f"   –ù–∞–π–¥–µ–Ω–æ URL –≤ sitemap: {sitemap_check['url_count']}")
        if sitemap_check['ok']:
            print("   ‚úÖ sitemap.xml –≤ –ø–æ—Ä—è–¥–∫–µ")
        else:
            print(f"   ‚ùå –ü—Ä–æ–±–ª–µ–º—ã: {', '.join(sitemap_check['errors'])}")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–¥–∏—Ä–µ–∫—Ç–æ–≤ –¥–ª—è –∫–ª—é—á–µ–≤—ã—Ö URL
        print("\nüîÑ –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–¥–∏—Ä–µ–∫—Ç–æ–≤ –¥–ª—è –∫–ª—é—á–µ–≤—ã—Ö URL...")
        redirect_issues = self.check_redirects_for_key_urls()
        if redirect_issues:
            print(f"   ‚ö†Ô∏è  –ù–∞–π–¥–µ–Ω–æ –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö —Ä–µ–¥–∏—Ä–µ–∫—Ç–æ–≤: {len(redirect_issues)}")
            for issue in redirect_issues:
                if issue.has_302:
                    print(f"      - {issue.start_url}: –Ω–∞–π–¥–µ–Ω—ã 302 —Ä–µ–¥–∏—Ä–µ–∫—Ç—ã (–¥–æ–ª–∂–Ω—ã –±—ã—Ç—å 301)")
                if issue.is_too_long:
                    print(f"      - {issue.start_url}: —Ü–µ–ø–æ—á–∫–∞ —Ä–µ–¥–∏—Ä–µ–∫—Ç–æ–≤ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω–∞—è ({len(issue.chain)} —Ä–µ–¥–∏—Ä–µ–∫—Ç–æ–≤)")
        else:
            print("   ‚úÖ –†–µ–¥–∏—Ä–µ–∫—Ç—ã –≤ –ø–æ—Ä—è–¥–∫–µ")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ URL –∏–∑ sitemap
        print(f"\nüìÑ –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ {max_urls} URL –∏–∑ –∫–∞–∂–¥–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ sitemap...")
        sitemap_urls = self.get_sitemap_urls()
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º URL –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        url_categories: Dict[str, List[str]] = {}
        for url in sitemap_urls:
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é –ø–æ –ø—É—Ç–∏
            parsed = urlparse(url)
            path = parsed.path
            
            if '/post/' in path:
                category = 'posts'
            elif '/project/' in path:
                category = 'projects'
            elif path in ['/en/', '/ru/', '/']:
                category = 'main_pages'
            elif '/quiz' in path or '/quizes' in path:
                category = 'quizzes'
            else:
                category = 'other'
            
            if category not in url_categories:
                url_categories[category] = []
            url_categories[category].append(url)
        
        pages_check = []
        for category, urls in url_categories.items():
            print(f"   –ü—Ä–æ–≤–µ—Ä–∫–∞ {category} ({min(len(urls), max_urls)} URL)...")
            for url in urls[:max_urls]:
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø—É—Ç—å –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –±–∞–∑–æ–≤–æ–≥–æ URL
                parsed = urlparse(url)
                path = parsed.path
                
                page_result = self.check_page(path, check_redirects=True)
                pages_check.append(page_result)
                
                # –í—ã–≤–æ–¥–∏–º –ø—Ä–æ–≥—Ä–µ—Å—Å –¥–ª—è –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü
                if page_result.errors or page_result.canonical_issues or page_result.missing_hreflang:
                    print(f"      ‚ö†Ô∏è  {path}: –ø—Ä–æ–±–ª–µ–º—ã –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã")
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        print("\nüìä –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...")
        status_codes = {}
        redirect_count = 0
        canonical_issues_count = 0
        hreflang_issues_count = 0
        
        for page in pages_check:
            status_codes[page.status_code] = status_codes.get(page.status_code, 0) + 1
            if page.redirect_chain and page.redirect_chain.chain:
                redirect_count += 1
            if page.canonical_issues:
                canonical_issues_count += 1
            if page.missing_hreflang:
                hreflang_issues_count += 1
        
        summary = {
            'total_pages_checked': len(pages_check),
            'status_codes': status_codes,
            'pages_with_redirects': redirect_count,
            'pages_with_canonical_issues': canonical_issues_count,
            'pages_with_hreflang_issues': hreflang_issues_count,
        }
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        recommendations = []
        
        if redirect_issues:
            recommendations.append(
                "–ò—Å–ø—Ä–∞–≤–∏—Ç—å 302 —Ä–µ–¥–∏—Ä–µ–∫—Ç—ã –Ω–∞ 301 –¥–ª—è SEO-–∫—Ä–∏—Ç–∏—á–Ω—ã—Ö URL "
                "(–∫–æ—Ä–Ω–µ–≤–æ–π URL, –ø–æ—Å—Ç—ã –±–µ–∑ —è–∑—ã–∫–æ–≤–æ–≥–æ –ø—Ä–µ—Ñ–∏–∫—Å–∞)"
            )
        
        if canonical_issues_count > 0:
            recommendations.append(
                f"–ò—Å–ø—Ä–∞–≤–∏—Ç—å canonical URL –Ω–∞ {canonical_issues_count} —Å—Ç—Ä–∞–Ω–∏—Ü–∞—Ö "
                "(–∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å https:// –∏ –æ—Å–Ω–æ–≤–Ω–æ–π –¥–æ–º–µ–Ω)"
            )
        
        if hreflang_issues_count > 0:
            recommendations.append(
                f"–î–æ–±–∞–≤–∏—Ç—å –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ hreflang —Ç–µ–≥–∏ –Ω–∞ {hreflang_issues_count} —Å—Ç—Ä–∞–Ω–∏—Ü–∞—Ö"
            )
        
        if sitemap_check.get('has_http_links'):
            recommendations.append(
                "–ó–∞–º–µ–Ω–∏—Ç—å –≤—Å–µ http:// —Å—Å—ã–ª–∫–∏ –Ω–∞ https:// –≤ sitemap.xml"
            )
        
        if not recommendations:
            recommendations.append("–í—Å–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø—Ä–æ–π–¥–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ! ‚úÖ")
        
        result = AuditResult(
            timestamp=datetime.now().isoformat(),
            base_url=self.base_url,
            robots_check=robots_check,
            sitemap_check=sitemap_check,
            redirect_issues=redirect_issues,
            pages_check=pages_check,
            summary=summary,
            recommendations=recommendations
        )
        
        return result


def format_report(result: AuditResult) -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –æ—Ç—á–µ—Ç –≤ —á–∏—Ç–∞–µ–º—ã–π –≤–∏–¥"""
    lines = []
    lines.append("=" * 80)
    lines.append("SEO –ê–£–î–ò–¢ –û–¢–ß–ï–¢")
    lines.append("=" * 80)
    lines.append(f"–î–∞—Ç–∞: {result.timestamp}")
    lines.append(f"–ë–∞–∑–æ–≤—ã–π URL: {result.base_url}")
    lines.append("")
    
    # Robots.txt
    lines.append("ROBOTS.TXT")
    lines.append("-" * 80)
    if result.robots_check['ok']:
        lines.append("‚úÖ robots.txt —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")
    else:
        lines.append("‚ùå –ü—Ä–æ–±–ª–µ–º—ã —Å robots.txt:")
        for error in result.robots_check['errors']:
            lines.append(f"   - {error}")
    lines.append("")
    
    # Sitemap
    lines.append("SITEMAP.XML")
    lines.append("-" * 80)
    lines.append(f"–í—Å–µ–≥–æ URL –≤ sitemap: {result.sitemap_check['url_count']}")
    if result.sitemap_check['ok']:
        lines.append("‚úÖ sitemap.xml —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")
    else:
        lines.append("‚ùå –ü—Ä–æ–±–ª–µ–º—ã —Å sitemap.xml:")
        for error in result.sitemap_check['errors']:
            lines.append(f"   - {error}")
    lines.append("")
    
    # –†–µ–¥–∏—Ä–µ–∫—Ç—ã
    lines.append("–†–ï–î–ò–†–ï–ö–¢–´")
    lines.append("-" * 80)
    if result.redirect_issues:
        lines.append(f"‚ö†Ô∏è  –ù–∞–π–¥–µ–Ω–æ –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö —Ä–µ–¥–∏—Ä–µ–∫—Ç–æ–≤: {len(result.redirect_issues)}")
        for issue in result.redirect_issues:
            lines.append(f"\n   URL: {issue.start_url}")
            lines.append(f"   –§–∏–Ω–∞–ª—å–Ω—ã–π URL: {issue.final_url}")
            lines.append(f"   –¶–µ–ø–æ—á–∫–∞ —Ä–µ–¥–∏—Ä–µ–∫—Ç–æ–≤ ({len(issue.chain)}):")
            for i, redirect in enumerate(issue.chain, 1):
                lines.append(f"      {i}. {redirect.status_code} ‚Üí {redirect.location}")
            if issue.has_302:
                lines.append("   ‚ö†Ô∏è  –í —Ü–µ–ø–æ—á–∫–µ –µ—Å—Ç—å 302 —Ä–µ–¥–∏—Ä–µ–∫—Ç—ã (–¥–æ–ª–∂–Ω—ã –±—ã—Ç—å 301)")
            if issue.is_too_long:
                lines.append("   ‚ö†Ô∏è  –¶–µ–ø–æ—á–∫–∞ —Ä–µ–¥–∏—Ä–µ–∫—Ç–æ–≤ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω–∞—è")
    else:
        lines.append("‚úÖ –ü—Ä–æ–±–ª–µ–º–Ω—ã—Ö —Ä–µ–¥–∏—Ä–µ–∫—Ç–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
    lines.append("")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü
    lines.append("–°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–†–û–í–ï–†–ï–ù–ù–´–• –°–¢–†–ê–ù–ò–¶")
    lines.append("-" * 80)
    lines.append(f"–í—Å–µ–≥–æ –ø—Ä–æ–≤–µ—Ä–µ–Ω–æ: {result.summary['total_pages_checked']}")
    lines.append(f"–° —Ä–µ–¥–∏—Ä–µ–∫—Ç–∞–º–∏: {result.summary['pages_with_redirects']}")
    lines.append(f"–° –ø—Ä–æ–±–ª–µ–º–∞–º–∏ canonical: {result.summary['pages_with_canonical_issues']}")
    lines.append(f"–° –ø—Ä–æ–±–ª–µ–º–∞–º–∏ hreflang: {result.summary['pages_with_hreflang_issues']}")
    lines.append("\n–°—Ç–∞—Ç—É—Å –∫–æ–¥—ã:")
    for status, count in sorted(result.summary['status_codes'].items()):
        lines.append(f"   {status}: {count}")
    lines.append("")
    
    # –ü—Ä–æ–±–ª–µ–º–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
    problematic_pages = [
        p for p in result.pages_check
        if p.errors or p.canonical_issues or p.missing_hreflang or (p.redirect_chain and p.redirect_chain.has_302)
    ]
    
    if problematic_pages:
        lines.append("–ü–†–û–ë–õ–ï–ú–ù–´–ï –°–¢–†–ê–ù–ò–¶–´")
        lines.append("-" * 80)
        for page in problematic_pages[:20]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 20
            lines.append(f"\n   {page.url}")
            if page.errors:
                for error in page.errors:
                    lines.append(f"      ‚ùå {error}")
            if page.canonical_issues:
                for issue in page.canonical_issues:
                    lines.append(f"      ‚ö†Ô∏è  Canonical: {issue}")
            if page.missing_hreflang:
                lines.append(f"      ‚ö†Ô∏è  –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç hreflang: {', '.join(page.missing_hreflang)}")
            if page.redirect_chain and page.redirect_chain.has_302:
                lines.append(f"      ‚ö†Ô∏è  –ù–∞–π–¥–µ–Ω—ã 302 —Ä–µ–¥–∏—Ä–µ–∫—Ç—ã –≤ —Ü–µ–ø–æ—á–∫–µ")
        if len(problematic_pages) > 20:
            lines.append(f"\n   ... –∏ –µ—â—ë {len(problematic_pages) - 20} —Å—Ç—Ä–∞–Ω–∏—Ü —Å –ø—Ä–æ–±–ª–µ–º–∞–º–∏")
    lines.append("")
    
    # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
    lines.append("–†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò")
    lines.append("-" * 80)
    for i, rec in enumerate(result.recommendations, 1):
        lines.append(f"{i}. {rec}")
    lines.append("")
    
    lines.append("=" * 80)
    
    return "\n".join(lines)


def main(argv: List[str] | None = None) -> int:
    parser = argparse.ArgumentParser(
        description="–ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π SEO-–∞—É–¥–∏—Ç –¥–ª—è quiz-code.com"
    )
    parser.add_argument(
        "--base-url",
        default=BASE_URL,
        help=f"–ë–∞–∑–æ–≤—ã–π URL —Å–∞–π—Ç–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: {BASE_URL})"
    )
    parser.add_argument(
        "--max-urls",
        type=int,
        default=MAX_URLS_PER_CATEGORY,
        help=f"–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ URL –∏–∑ –∫–∞–∂–¥–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: {MAX_URLS_PER_CATEGORY})"
    )
    parser.add_argument(
        "--output",
        help="–ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ—Ç—á–µ—Ç–∞ (JSON)"
    )
    parser.add_argument(
        "--report",
        help="–ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –æ—Ç—á–µ—Ç–∞"
    )
    
    args = parser.parse_args(argv)
    
    auditor = SEOAuditor(base_url=args.base_url)
    result = auditor.run_audit(max_urls=args.max_urls)
    
    # –í—ã–≤–æ–¥–∏–º –∫—Ä–∞—Ç–∫–∏–π –æ—Ç—á–µ—Ç –≤ –∫–æ–Ω—Å–æ–ª—å
    print("\n" + "=" * 80)
    print("–ö–†–ê–¢–ö–ò–ô –û–¢–ß–ï–¢")
    print("=" * 80)
    print(format_report(result))
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º JSON –æ—Ç—á–µ—Ç
    if args.output:
        with open(args.output, 'w', encoding='utf-8') as f:
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º dataclass –≤ dict –¥–ª—è —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–∏
            result_dict = {
                'timestamp': result.timestamp,
                'base_url': result.base_url,
                'robots_check': result.robots_check,
                'sitemap_check': result.sitemap_check,
                'redirect_issues': [
                    {
                        'start_url': issue.start_url,
                        'final_url': issue.final_url,
                        'chain': [asdict(r) for r in issue.chain],
                        'is_too_long': issue.is_too_long,
                        'has_302': issue.has_302,
                    }
                    for issue in result.redirect_issues
                ],
                'pages_check': [
                    {
                        'url': page.url,
                        'status_code': page.status_code,
                        'has_canonical': page.has_canonical,
                        'canonical_url': page.canonical_url,
                        'canonical_issues': page.canonical_issues,
                        'hreflang_tags': page.hreflang_tags,
                        'missing_hreflang': page.missing_hreflang,
                        'has_robots_meta': page.has_robots_meta,
                        'robots_content': page.robots_content,
                        'errors': page.errors,
                    }
                    for page in result.pages_check
                ],
                'summary': result.summary,
                'recommendations': result.recommendations,
            }
            json.dump(result_dict, f, ensure_ascii=False, indent=2)
        print(f"\n‚úÖ JSON –æ—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {args.output}")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–π –æ—Ç—á–µ—Ç
    if args.report:
        with open(args.report, 'w', encoding='utf-8') as f:
            f.write(format_report(result))
        print(f"‚úÖ –¢–µ–∫—Å—Ç–æ–≤—ã–π –æ—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {args.report}")
    
    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–æ–¥ –≤—ã—Ö–æ–¥–∞
    if result.recommendations and not all("‚úÖ" in rec for rec in result.recommendations):
        return 1
    return 0


if __name__ == "__main__":
    sys.exit(main())

